{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if GPU is available.\n",
      "GPU is available and being used.\n",
      "Current working directory: /home/aaron/git/ImageColorization\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "print(\"Testing if GPU is available.\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    Warning(\"GPU is not available, Falling back to CPU.\")\n",
    "\n",
    "os.chdir(\"/home/aaron/git/ImageColorization/\")\n",
    "BASEADDR = os.getcwd()\n",
    "print(f\"Current working directory: {BASEADDR}\")\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unpack imagenet64 database\n",
    "imagenet64 is the imagenet database with each image scaled to be 64 by 64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready!\n"
     ]
    }
   ],
   "source": [
    "checkpoints = f\"{BASEADDR}/checkpoints/\"\n",
    "content = f\"{BASEADDR}/content/\"\n",
    "if not os.path.exists(checkpoints):\n",
    "    os.makedirs(checkpoints)\n",
    "\n",
    "import shutil\n",
    "if not os.path.exists(content):\n",
    "    os.makedirs(content)\n",
    "if not os.path.exists(f'{content}/imagenet64'):\n",
    "  os.chdir(content)\n",
    "  if not os.path.exists(content + 'imagenet64.tar'):\n",
    "    print(\"Downloading archive...\")\n",
    "    !wget https://pjreddie.com/media/files/imagenet64.tar\n",
    "  print(\"Uncompressing...\")\n",
    "  !tar -xf imagenet64.tar\n",
    "\n",
    "print(\"Data ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load color image\n",
    "        color_img, _ = self.dataset[idx]\n",
    "\n",
    "        # Convert color image to grayscale\n",
    "        gray = transforms.Grayscale()\n",
    "        grayscale_img = gray(color_img)\n",
    "\n",
    "        return grayscale_img, color_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imagenet64_data(batchSize):\n",
    "  transform=transforms.Compose([\n",
    "                             transforms.RandomResizedCrop(64, scale=(1.0, 1.0), ratio=(1., 1.)),\n",
    "                              transforms.RandomHorizontalFlip(),\n",
    "                              transforms.ToTensor()\n",
    "                          ])\n",
    "\n",
    "  # load training dataset\n",
    "  trainSet = torchvision.datasets.ImageFolder(root=f'{content}/imagenet64/train/', transform=transform)\n",
    "  colorizationTrainSet = ColorizationDataset(trainSet)\n",
    "  trainLoader = DataLoader(colorizationTrainSet, batch_size=batchSize, shuffle=True, num_workers=0)\n",
    "\n",
    "  # load testing dataset\n",
    "  testSet = torchvision.datasets.ImageFolder(root=f'{content}/imagenet64/val/', transform=transform)\n",
    "  colorizationTestSet = ColorizationDataset(testSet)\n",
    "  testloader = torch.utils.data.DataLoader(colorizationTestSet, batch_size=batchSize, shuffle=False, num_workers=0)\n",
    "  \n",
    "  return {'train': trainLoader, 'test': testloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrayNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      "  (output_layer): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0392,  0.0582,  0.0598,  ...,  0.0602,  0.0527,  0.0504],\n",
       "          [ 0.0452,  0.0593,  0.0416,  ...,  0.0509,  0.0637,  0.0382],\n",
       "          [ 0.0475,  0.0503,  0.0594,  ...,  0.0703,  0.0614,  0.0305],\n",
       "          ...,\n",
       "          [ 0.0395,  0.0643,  0.0732,  ...,  0.0751,  0.0734,  0.0509],\n",
       "          [ 0.0392,  0.0547,  0.0733,  ...,  0.0626,  0.0678,  0.0472],\n",
       "          [ 0.0170,  0.0249,  0.0297,  ...,  0.0311,  0.0274,  0.0123]],\n",
       "\n",
       "         [[ 0.0546,  0.0807,  0.0931,  ...,  0.1022,  0.0794,  0.0819],\n",
       "          [ 0.0512,  0.0805,  0.0731,  ...,  0.0640,  0.0749,  0.0650],\n",
       "          [ 0.0624,  0.0781,  0.0784,  ...,  0.0694,  0.0844,  0.0555],\n",
       "          ...,\n",
       "          [ 0.0644,  0.0803,  0.0764,  ...,  0.0646,  0.0648,  0.0706],\n",
       "          [ 0.0593,  0.0743,  0.0758,  ...,  0.0849,  0.0788,  0.0729],\n",
       "          [ 0.0580,  0.0479,  0.0462,  ...,  0.0579,  0.0438,  0.0446]],\n",
       "\n",
       "         [[-0.0552, -0.0425, -0.0590,  ..., -0.0475, -0.0463, -0.0575],\n",
       "          [-0.0528, -0.0509, -0.0377,  ..., -0.0307, -0.0470, -0.0636],\n",
       "          [-0.0494, -0.0320, -0.0336,  ..., -0.0338, -0.0557, -0.0579],\n",
       "          ...,\n",
       "          [-0.0575, -0.0445, -0.0487,  ..., -0.0402, -0.0477, -0.0660],\n",
       "          [-0.0598, -0.0681, -0.0493,  ..., -0.0454, -0.0548, -0.0750],\n",
       "          [-0.0493, -0.0646, -0.0631,  ..., -0.0626, -0.0728, -0.0835]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GrayNet(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_output_channels, initial_channels=16):\n",
    "        super(GrayNet, self).__init__()\n",
    "\n",
    "        # Initial number of channels\n",
    "        in_channels = num_input_channels\n",
    "\n",
    "        # Define the initial layers of the network\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(num_input_channels, initial_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "\n",
    "        # Update the number of input channels\n",
    "        in_channels = initial_channels\n",
    "\n",
    "        # Add more layers based on your desired architecture\n",
    "        for _ in range(3):  # You can customize the number of layers\n",
    "            self.layers.extend([\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Conv2d(in_channels, num_output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Final output\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "testInput = torch.randn(1,1,64,64).to(device)\n",
    "testNetwork = GrayNet(1,3).to(device)\n",
    "print(testNetwork)\n",
    "testNetwork(testInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_architecture(parent_architecture):\n",
    "    # Copy the parent architecture\n",
    "    child_architecture = parent_architecture.copy()\n",
    "\n",
    "    # Randomly select an operation (e.g., add a layer, remove a layer, change a layer's parameters)\n",
    "    operation = np.random.choice([\"add_layer\", \"remove_layer\", \"modify_layer\"])\n",
    "\n",
    "    if operation == \"add_layer\":\n",
    "        # Add a new layer to the architecture\n",
    "        child_architecture.append(nn.Conv2d(16, 16, kernel_size=3, padding=1))  # Define the new layer\n",
    "\n",
    "    elif operation == \"remove_layer\":\n",
    "        # Remove a random layer from the architecture\n",
    "        if len(child_architecture) > 1:  # Ensure there's at least one layer\n",
    "            index_to_remove = np.random.choice(len(child_architecture))\n",
    "            del child_architecture[index_to_remove]\n",
    "\n",
    "    elif operation == \"modify_layer\":\n",
    "        # Modify the parameters of a random layer in the architecture\n",
    "        if len(child_architecture) > 0:\n",
    "            index_to_modify = np.random.choice(len(child_architecture))\n",
    "            # Modify the parameters of the layer, e.g., change kernel size, channels, etc.\n",
    "\n",
    "    return child_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(architecture, data_loader, num_epochs=5):\n",
    "    # Assuming you have access to labeled training data and a loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(architecture.parameters(), lr=0.001)\n",
    "\n",
    "    architecture.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in tqdm(data_loader, leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = architecture(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # After training, assess the performance on a validation set or the entire dataset\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader, leave=False):\n",
    "            outputs = architecture(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    return -average_loss  # Return the negative loss as a fitness score for maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evolutionary_algorithm(population_size, generations, data_loader):\n",
    "    population = [GrayNet(1,3) for _ in range(population_size)]\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"Generation {generation + 1}/{generations}\")\n",
    "\n",
    "        # Evaluate each individual in the population\n",
    "        fitness_scores = []\n",
    "        for individual in population:\n",
    "            fitness = evaluate(individual.to(device), data_loader)\n",
    "            fitness_scores.append(fitness)\n",
    "\n",
    "        # Select the top performers\n",
    "        top_indices = np.argsort(fitness_scores)[-int(0.2 * population_size):]\n",
    "\n",
    "        # Create a new population through crossover and mutation\n",
    "        new_population = [population[i] for i in top_indices]\n",
    "\n",
    "        while len(new_population) < population_size:\n",
    "            # Randomly select parents from the top performers\n",
    "            parent1 = population[np.random.choice(top_indices)]\n",
    "            parent2 = population[np.random.choice(top_indices)]\n",
    "\n",
    "            # Crossover and mutation\n",
    "            child_architecture = mutate_architecture(parent1.architecture + parent2.architecture)\n",
    "            child = GrayNet(1,3)  # Initialize the child with the mutated architecture\n",
    "            new_population.append(child)\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "    # Return the best individual\n",
    "    best_index = np.argmax(fitness_scores)\n",
    "    return population[best_index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431cc3eb4ffc440796198881f64dda12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainLoader \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m testLoader \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m best_individual \u001b[39m=\u001b[39m evolutionary_algorithm(population_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, generations\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, data_loader\u001b[39m=\u001b[39;49mtestLoader)\n",
      "\u001b[1;32m/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m fitness_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m individual \u001b[39min\u001b[39;00m population:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     fitness \u001b[39m=\u001b[39m evaluate(individual\u001b[39m.\u001b[39;49mto(device), data_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     fitness_scores\u001b[39m.\u001b[39mappend(fitness)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Select the top performers\u001b[39;00m\n",
      "\u001b[1;32m/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         outputs \u001b[39m=\u001b[39m architecture(inputs\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(outputs, targets\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/EvolutionNetwork.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# After training, assess the performance on a validation set or the entire dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = get_imagenet64_data(BATCH_SIZE)\n",
    "trainLoader = data['train']\n",
    "testLoader = data['test']\n",
    "best_individual = evolutionary_algorithm(population_size=10, generations=5, data_loader=testLoader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
