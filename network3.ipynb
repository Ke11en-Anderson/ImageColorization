{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi0K48gAy0qC"
      },
      "source": [
        "Colorspace and architecture: https://arxiv.org/abs/2204.02850  \n",
        "Fully Convolutional Networks: https://arxiv.org/pdf/1411.4038.pdf\n",
        "\n",
        "Jetson nano: 0.236 TFLOPS fp32\n",
        "\n",
        "Image colorization on youtube: https://youtu.be/WXyeQeHUxpc?si=jQfcU8Ra4StxFOwT\n",
        "\n",
        "Youtube code: https://colab.research.google.com/drive/1BsqM7GBTtsyBixy2jsLGJNiSvp1ocpV7?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujhLAO62qn4r",
        "outputId": "6dcd41ce-cf9a-4575-b5d9-caf3575a535a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing if GPU is available.\n",
            "GPU is available and being used.\n",
            "Current working directory: /home/aaron/git/ImageColorization\n"
          ]
        }
      ],
      "source": [
        "! pip install torchprofile 1>/dev/null\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import OrderedDict, defaultdict\n",
        "from typing import Union, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchprofile import profile_macs # Helps us to obtain mac calculations\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "print(\"Testing if GPU is available.\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available and being used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, Falling back to CPU.\")\n",
        "\n",
        "os.chdir(\"/home/aaron/git/ImageColorization/\")\n",
        "BASEADDR = os.getcwd()\n",
        "print(f\"Current working directory: {BASEADDR}\")\n",
        "\n",
        "BATCH_SIZE = 32\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download and unpack imagenet64 database\n",
        "imagenet64 is the imagenet database with each image scaled to be 64 by 64 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_q0Y2ZjQ7ts",
        "outputId": "1027aefb-0575-4141-9ab0-0d42bccccdbc"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoints = f\"{BASEADDR}/checkpoints/VGG13based\"\n",
        "content = f\"/media/aaron/Storage/imagenet/ILSVRC/Data/CLS-LOC\"\n",
        "if not os.path.exists(content):\n",
        "    raise Exception(\"Mount the hard drive moron.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "esNpGo4JEj-b",
        "outputId": "87e2aa8f-a1af-49ba-afa6-0135a3b88c44"
      },
      "outputs": [],
      "source": [
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load color image\n",
        "        color_img, _ = self.dataset[idx]\n",
        "\n",
        "        # Convert color image to grayscale\n",
        "        gray = transforms.Grayscale()\n",
        "        graytmp = gray(color_img)\n",
        "        grayscale_img = torch.cat((graytmp,graytmp,graytmp))\n",
        "\n",
        "        return grayscale_img, color_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PZTbzChXHF17"
      },
      "outputs": [],
      "source": [
        "def get_imagenet_data(batchSize):\n",
        "  ## transformations\n",
        "  ##transform = transforms.Compose(\n",
        "    ##  [transforms.Resize(64), transforms.RandomHorizontalFlip(), transforms.ToTensor().to(device)])\n",
        "\n",
        "  transform=transforms.Compose([\n",
        "                              transforms.RandomResizedCrop((640,480), scale=(1.0, 1.0), ratio=(1., 1.)),\n",
        "                              transforms.RandomHorizontalFlip(),\n",
        "                              transforms.ToTensor(),\n",
        "                          ])\n",
        "\n",
        "  ## download and load training dataset\n",
        "  trainSet = torchvision.datasets.ImageFolder(root=f'{content}/train/', transform=transform)\n",
        "  colorizationTrainSet = ColorizationDataset(trainSet)\n",
        "  trainLoader = DataLoader(colorizationTrainSet, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "  ## download and load testing dataset\n",
        "  testSet = torchvision.datasets.ImageFolder(root=f'{content}/val/', transform=transform)\n",
        "  colorizationTestSet = ColorizationDataset(testSet)\n",
        "  testloader = torch.utils.data.DataLoader(colorizationTestSet, batch_size=batchSize, shuffle=False, num_workers=2, pin_memory=True)\n",
        "  return {'train': trainLoader, 'test': testloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GrayNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv11): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv12): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv15): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv16): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv19): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv20): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv23): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv25): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv26): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn24): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn26): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "tensor([[[[0.0000, 0.0000, 0.5515,  ..., 0.8390, 0.6535, 0.1674],\n",
            "          [0.0000, 0.5251, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.4914, 0.0000,  ..., 0.4545, 0.0000, 0.5816],\n",
            "          ...,\n",
            "          [0.0000, 1.0478, 0.0000,  ..., 0.0000, 0.0000, 0.5747],\n",
            "          [0.0000, 0.3268, 0.0000,  ..., 0.4224, 1.2606, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.7567, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.9870],\n",
            "          [0.0000, 0.0000, 0.2618,  ..., 0.3330, 1.7243, 0.2839],\n",
            "          [0.1211, 0.0000, 0.0000,  ..., 0.9537, 1.1095, 0.4728],\n",
            "          ...,\n",
            "          [0.2827, 0.8058, 0.0000,  ..., 0.0000, 1.2276, 0.5881],\n",
            "          [1.0929, 0.5002, 1.2304,  ..., 1.3532, 0.3657, 0.0000],\n",
            "          [1.2070, 0.0000, 1.1703,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.3381, 0.6306, 0.0000,  ..., 0.0000, 0.6694, 0.0000],\n",
            "          [0.0000, 0.6435, 0.0368,  ..., 1.2478, 0.5837, 1.9577],\n",
            "          [0.1081, 0.3593, 1.1307,  ..., 1.1812, 0.8467, 0.9297],\n",
            "          ...,\n",
            "          [0.2528, 0.0000, 0.0000,  ..., 0.0000, 1.0354, 0.7820],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.2134, 0.9864, 0.0967],\n",
            "          [0.4388, 0.0000, 0.1661,  ..., 0.0000, 1.9169, 0.1895]]]],\n",
            "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Adjusted network to match VGG13\n",
        "class GrayNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GrayNet, self).__init__()\n",
        "        channels = 64\n",
        "        #block 1\n",
        "        self.conv1 = nn.Conv2d(3, channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        #block 2\n",
        "        self.conv3 = nn.Conv2d(channels, channels*2, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(channels*2, channels*2, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(channels*2)\n",
        "        self.bn4 = nn.BatchNorm2d(channels*2)\n",
        "\n",
        "        #block 3\n",
        "        self.conv5 = nn.Conv2d(channels*2, channels*4, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(channels*4, channels*4, 3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(channels*4)\n",
        "        self.bn6 = nn.BatchNorm2d(channels*4)\n",
        "\n",
        "        #block 4\n",
        "        self.conv7 = nn.Conv2d(channels*4, channels*8, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(channels*8, channels*8, 3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(channels*8)\n",
        "        self.bn8 = nn.BatchNorm2d(channels*8)\n",
        "\n",
        "        #block 5\n",
        "        self.conv9 = nn.Conv2d(channels*8, channels*8, 3, padding=1)\n",
        "        self.conv10 = nn.Conv2d(channels*8, channels*8, 3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(channels*8)\n",
        "        self.bn10 = nn.BatchNorm2d(channels*8)\n",
        "        self.conv11 = nn.ConvTranspose2d(channels*8,channels*8, kernel_size=2,stride=2)\n",
        "\n",
        "        #block 6\n",
        "        self.conv12 = nn.Conv2d(channels*16, channels*4, 1, padding=0)\n",
        "        self.conv13 = nn.Conv2d(channels*4, channels*4, 3, padding=1)\n",
        "        self.conv14 = nn.Conv2d(channels*4, channels*4, 3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(channels*4)\n",
        "        self.bn13 = nn.BatchNorm2d(channels*4)\n",
        "        self.bn14 = nn.BatchNorm2d(channels*4)\n",
        "        self.conv15 = nn.ConvTranspose2d(channels*4,channels*4, kernel_size=2,stride=2)\n",
        "\n",
        "        #block 7\n",
        "        self.conv16 = nn.Conv2d(channels*8, channels*2, 1, padding=0)\n",
        "        self.conv17 = nn.Conv2d(channels*2, channels*2, 3, padding=1)\n",
        "        self.conv18 = nn.Conv2d(channels*2, channels*2, 3, padding=1)\n",
        "        self.bn16 = nn.BatchNorm2d(channels*2)\n",
        "        self.bn17 = nn.BatchNorm2d(channels*2)\n",
        "        self.bn18 = nn.BatchNorm2d(channels*2)\n",
        "        self.conv19 = nn.ConvTranspose2d(channels*2,channels*2, kernel_size=2,stride=2)\n",
        "\n",
        "        #block 8\n",
        "        self.conv20 = nn.Conv2d(channels*4, channels, 1, padding=0)\n",
        "        self.conv21 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.conv22 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.bn20 = nn.BatchNorm2d(channels)\n",
        "        self.bn21 = nn.BatchNorm2d(channels)\n",
        "        self.bn22 = nn.BatchNorm2d(channels)\n",
        "        self.conv23 = nn.ConvTranspose2d(channels,channels, kernel_size=2,stride=2)\n",
        "\n",
        "        #block 9\n",
        "        self.conv24 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.conv25 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.conv26 = nn.Conv2d(channels, 3, 3, padding=1)\n",
        "        self.bn24 = nn.BatchNorm2d(channels)\n",
        "        self.bn25 = nn.BatchNorm2d(channels)\n",
        "        self.bn26 = nn.BatchNorm2d(3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.size())\n",
        "        # Input 64x64x3\n",
        "        #block1\n",
        "        x = F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x)))))) # 32x32xchannels\n",
        "        block1 = x\n",
        "        x = F.max_pool2d(x,2)\n",
        "        \n",
        "        #block2\n",
        "        x = F.relu(self.bn4(self.conv4(F.relu(self.bn3(self.conv3(x)))))) # 16x16xchannels*2\n",
        "        block2 = x\n",
        "        x = F.max_pool2d(x,2)\n",
        "        \n",
        "        #block3\n",
        "        x = F.relu(self.bn6(self.conv6(F.relu(self.bn5(self.conv5(x)))))) # 8x8xchannels*4\n",
        "        block3 = x\n",
        "        x = F.max_pool2d(x,2)\n",
        "        \n",
        "        #block4\n",
        "        x = F.relu(self.bn8(self.conv8(F.relu(self.bn7(self.conv7(x)))))) # 4x4xchannels*8\n",
        "        block4 = x\n",
        "        x = F.max_pool2d(x,2)\n",
        "        \n",
        "        #block5\n",
        "        x = self.conv11(F.relu(self.bn10(self.conv10(F.relu(self.bn9(self.conv9(x))))))) # 8x8xchannels*8\n",
        "\n",
        "        #block6\n",
        "        x = torch.cat((x,block4),1) #8x8xchannels*16\n",
        "        x = self.conv15(F.relu(self.bn14(self.conv14(F.relu(self.bn13(self.conv13(F.relu(self.bn12(self.conv12(x)))))))))) #16x16xchannels*4\n",
        "        \n",
        "        #block7\n",
        "        x =torch.cat((x,block3),1)#16x16xchannels*8\n",
        "        x = self.conv19(F.relu(self.bn18(self.conv18(F.relu(self.bn17(self.conv17(F.relu(self.bn16(self.conv16(x)))))))))) #32x32xchannels*2\n",
        "        \n",
        "        #block8\n",
        "        x =torch.cat((x,block2),1) #32x32xchannels*4\n",
        "        x = self.conv23(F.relu(self.bn22(self.conv22(F.relu(self.bn21(self.conv21(F.relu(self.bn20(self.conv20(x)))))))))) #64x64xchannels\n",
        "        \n",
        "        #block9\n",
        "        x = F.relu(self.bn26(self.conv26(F.relu(self.bn25(self.conv25(F.relu(self.bn24(self.conv24(x))))))))) #64x64x3 RGB\n",
        "        return x\n",
        "testInput = torch.randn(1,3,64,64).to(device)\n",
        "testNetwork = GrayNet().to(device)\n",
        "print(testNetwork)\n",
        "print(testNetwork(testInput))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainEvaluate(architecture, data_loader, test_loader, num_epochs=5):\n",
        "    # Assuming you have access to labeled training data and a loss function\n",
        "    criterion = nn.L1Loss()#nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(architecture.parameters(), lr=0.001)\n",
        "\n",
        "    architecture.to(device, non_blocking=True)\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"\\tepoch: \", epoch+1, \"/\", num_epochs)\n",
        "        for inputs, targets in tqdm(data_loader, leave=False):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = architecture(inputs.to(device, non_blocking=True))\n",
        "            loss = criterion(outputs, targets.to(device, non_blocking=True))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # After training, assess the performance on a validation set or the entire dataset\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_loader, leave=False):\n",
        "            outputs = architecture(inputs.to(device, non_blocking=True))\n",
        "            loss = criterion(outputs, targets.to(device, non_blocking=True))\n",
        "            total_loss += loss.item() * len(targets)\n",
        "            total_samples += len(targets)\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    return -average_loss  # Return the negative loss as a fitness score for maximization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize model and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any class folder in /media/aaron/Storage/imagenet/ILSVRC/Data/CLS-LOC/val/.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/aaron/git/ImageColorization/network3.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m get_imagenet_data(BATCH_SIZE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainLoader \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m testLoader \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[1;32m/home/aaron/git/ImageColorization/network3.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainLoader \u001b[39m=\u001b[39m DataLoader(colorizationTrainSet, batch_size\u001b[39m=\u001b[39mbatchSize, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m## download and load testing dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m testSet \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mImageFolder(root\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mcontent\u001b[39m}\u001b[39;49;00m\u001b[39m/val/\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m colorizationTestSet \u001b[39m=\u001b[39m ColorizationDataset(testSet)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aaron/git/ImageColorization/network3.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m testloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(colorizationTestSet, batch_size\u001b[39m=\u001b[39mbatchSize, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    310\u001b[0m         root,\n\u001b[1;32m    311\u001b[0m         loader,\n\u001b[1;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /media/aaron/Storage/imagenet/ILSVRC/Data/CLS-LOC/val/."
          ]
        }
      ],
      "source": [
        "data = get_imagenet_data(BATCH_SIZE)\n",
        "trainLoader = data['train']\n",
        "testLoader = data['test']\n",
        "\n",
        "model = GrayNet().to(device)\n",
        "# Load model from file.\n",
        "loadPath = f\"{BASEADDR}/checkpoints/finaltrainedweights.pt\"\n",
        "model.load_state_dict(torch.load(loadPath))\n",
        "recover_model = lambda: model.load_state_dict(torch.load(loadPath))\n",
        "# # Load pretrained weights into the decoder portion of the network\n",
        "# pre = torchvision.models.vgg13_bn(weights = torchvision.models.VGG13_BN_Weights).to(device)\n",
        "# params = pre.features[0].state_dict()\n",
        "# model.conv1.load_state_dict(params)\n",
        "# model.conv1.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[3].state_dict()\n",
        "# model.conv2.load_state_dict(params)\n",
        "# model.conv2.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[7].state_dict()\n",
        "# model.conv3.load_state_dict(params)\n",
        "# model.conv3.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[10].state_dict()\n",
        "# model.conv4.load_state_dict(params)\n",
        "# model.conv4.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[14].state_dict()\n",
        "# model.conv5.load_state_dict(params)\n",
        "# model.conv5.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[17].state_dict()\n",
        "# model.conv6.load_state_dict(params)\n",
        "# model.conv6.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[21].state_dict()\n",
        "# model.conv7.load_state_dict(params)\n",
        "# model.conv7.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[24].state_dict()\n",
        "# model.conv8.load_state_dict(params)\n",
        "# model.conv8.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[28].state_dict()\n",
        "# model.conv9.load_state_dict(params)\n",
        "# model.conv9.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later\n",
        "# params = pre.features[31].state_dict()\n",
        "# model.conv10.load_state_dict(params)\n",
        "# model.conv10.requires_grad_ = False  #Freeze layer for transfer learning. May want to unfreeze later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save model to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# Save current model to file\n",
        "def saveModel(state, savePath):\n",
        "    if not os.path.exists(os.path.dirname(savePath)):\n",
        "        os.makedirs(os.path.dirname(savePath))\n",
        "    torch.save(state, savePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load existing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the data fed into the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ZB04pOeErVjQ",
        "outputId": "fe9b1906-2abf-4653-8a64-f199507f19e3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "def visualizeData(dataLoader: DataLoader):\n",
        "    inputs, targets = next(iter(dataLoader))\n",
        "\n",
        "    # show grayscale images.\n",
        "    imshow(torchvision.utils.make_grid(inputs))\n",
        "    \n",
        "    # show original images.\n",
        "    imshow(torchvision.utils.make_grid(targets))\n",
        "\n",
        "visualizeData(trainLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "def visualizeNetwork(model: nn.Module, dataLoader: DataLoader):\n",
        "    inputs, targets = next(iter(dataLoader))\n",
        "\n",
        "    # show grayscale images.\n",
        "    imshow(torchvision.utils.make_grid(inputs))\n",
        "\n",
        "    # show colorized images.\n",
        "    outputs = model(inputs.to(device))\n",
        "    imshow(torchvision.utils.make_grid(outputs.cpu()))\n",
        "    \n",
        "    # show original images.\n",
        "    imshow(torchvision.utils.make_grid(targets))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "iteration = 0\n",
        "\n",
        "bestAccuracy = 0\n",
        "bestState = model.state_dict()\n",
        "\n",
        "while True:\n",
        "    recover_model()\n",
        "    accuracy = trainEvaluate(model, trainLoader, testLoader)\n",
        "    if accuracy > bestAccuracy:\n",
        "        bestAccuracy = accuracy\n",
        "        save_path = f\"{checkpoints}/{iteration}.pt\"\n",
        "        saveModel(model.state_dict(), save_path)\n",
        "        bestState = model.state_dict()\n",
        "        recover_model = lambda: model.load_state_dict(bestState)\n",
        "    print(\"Iteration: \", iteration, \" had accuracy of \", accuracy)\n",
        "    visualizeNetwork(model, trainLoader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "def compareNetworks(model: nn.Module, dataLoader: DataLoader, net1: str, net2: str):\n",
        "    data = get_imagenet_data(8)\n",
        "    inputs, targets = next(iter(data['train']))\n",
        "\n",
        "    # Save original state of model.\n",
        "    originalState = model.state_dict()\n",
        "\n",
        "    # get colorized images from networks.\n",
        "    model.load_state_dict(torch.load(net1))\n",
        "    outputs1 = model(inputs.to(device))\n",
        "    model.load_state_dict(torch.load(net2))\n",
        "    outputs2 = model(inputs.to(device))\n",
        "\n",
        "    # Display colorized images alongside true image.\n",
        "    imshow(torchvision.utils.make_grid(torch.cat((outputs1.cpu(), outputs2.cpu(), targets),2)))\n",
        "    \n",
        "    # Load original state back into model.\n",
        "    model.load_state_dict(originalState)\n",
        "\n",
        "compareNetworks(model, trainLoader, f\"{checkpoints}0/0.pt\", f\"{checkpoints}0/16.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
